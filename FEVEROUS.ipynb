{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonlines\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "sys.path.insert(0, \"e:\\\\Documents\\\\NLP\\\\FEVER2021_SharedTask\\\\FEVEROUS\\\\src\")\n",
    "\n",
    "DIR_PATH = \"e:\\\\Documents\\\\NLP\\\\FEVER2021_SharedTask\\\\\"\n",
    "TRAIN_DATA_PATH = os.path.join(DIR_PATH, 'data\\\\train.jsonl')\n",
    "\n",
    "from database.feverous_db import FeverousDB\n",
    "from utils.wiki_page import WikiPage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc36b9",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FeverousDB(\"C:/Databases/feverous_wikiv1.db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ff17c",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with jsonlines.open(TRAIN_DATA_PATH) as reader:\n",
    "    for i, doc in enumerate(reader):\n",
    "        train_data.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_example = train_data[0]\n",
    "evidence = train_example['evidence']\n",
    "PAGE_NAME = \"Tammy Garcia\"\n",
    "page_json = db.get_doc_json(PAGE_NAME)\n",
    "wiki_page = WikiPage(PAGE_NAME, page_json)\n",
    "\n",
    "def get_sent_evidence(train_json):    \n",
    "    for e in evidence:\n",
    "        sent_ids = []\n",
    "        content = e['content']\n",
    "        for c in content:\n",
    "            sent_id = c.replace(PAGE_NAME + \"_\", \"\")\n",
    "            sent_ids.append(sent_id)\n",
    "\n",
    "    sentences = wiki_page.get_sentences()\n",
    "    content_text = \"\"\n",
    "    for sent_id in sent_ids:\n",
    "        for sent in sentences:\n",
    "            if sent.name == sent_id:\n",
    "                content_text += sent.content + \" \"\n",
    "                break\n",
    "\n",
    "print(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f16d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "doc_ids = db.get_doc_ids()\n",
    "print(\"Nr of docs: {} took {} seconds to fetch\".format(len(doc_ids), time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf68248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from glob import glob\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "\n",
    "CORPUS_PATH = DIR_PATH + 'data\\\\corpora\\\\'\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def create_corpus():\n",
    "    file_paths = glob(CORPUS_PATH + '*.json')\n",
    "    for f_path in file_paths:\n",
    "        print(\"Opening file '{}'\".format(f_path))\n",
    "        with open(f_path, 'r') as f:\n",
    "            docs = json.loads(f.read())\n",
    "            for key in docs:\n",
    "                yield docs[key]\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "\n",
    "start_time = time.time()\n",
    "# Without stemming\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words='english',dtype=np.float32)\n",
    "# With stemming\n",
    "# tfidfvectorizer = TfidfVectorizer(stop_words='english', tokenizer=stemming_tokenizer)\n",
    "corpus = create_corpus()\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(corpus)\n",
    "print(\"Creating TF-IDF matrix took {} seconds\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdbac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "CORPUS_PATH = DIR_PATH + 'data\\\\corpora\\\\'\n",
    "\n",
    "def create_doc_id_map():\n",
    "    doc_id_map = []\n",
    "    file_paths = glob(CORPUS_PATH + '*.json')\n",
    "    for f_path in file_paths:\n",
    "        with open(f_path, 'r') as f:\n",
    "            docs = json.loads(f.read())\n",
    "            for key in docs:\n",
    "                doc_id_map.append(key)\n",
    "    return doc_id_map\n",
    "\n",
    "doc_id_map = create_doc_id_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0dfbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d19feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wm.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getsizeof(doc_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05978d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(tfidfvectorizer, open(\"vectorizer-32bit.pickle\", \"wb\"))\n",
    "pickle.dump(tfidf_wm, open(\"tfidf_wm-32bit.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "tfidfvectorizer = pickle.load(open(\"vectorizer-32bit.pickle\", \"rb\"))\n",
    "tfidf_wm = pickle.load(open(\"tfidf_wm-32bit.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb96ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DIR_PATH + 'data\\\\doc_id_map.json', 'r') as f:\n",
    "    doc_id_map = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4050114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "srp = SparseRandomProjection()\n",
    "tfidf_wm_reduced = srp.fit_transform(tfidf_wm)\n",
    "tfidf_wm_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_wm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c779183",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = train_data[0]['claim']\n",
    "query_tfidf = tfidfvectorizer.transform([test_query])\n",
    "cosine_similarities = cosine_similarity(query_tfidf, tfidf_wm).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a77142",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities[:-6:-1]\n",
    "# cosine_similarities.sort()[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21174eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "related_docs_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id_map[4244298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import trange\n",
    "\n",
    "def get_top_docs(claim):\n",
    "    query_tfidf = tfidfvectorizer.transform([claim])\n",
    "    cosine_similarities = cosine_similarity(query_tfidf, tfidf_wm).flatten()\n",
    "    related_docs_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "    return [doc_id_map[i] for i in related_docs_indices]\n",
    "\n",
    "TEST_SAMPLE = 10\n",
    "claim_top_docs = []\n",
    "for i in trange(TEST_SAMPLE):\n",
    "    claim = train_data[i]['claim']\n",
    "    claim_top_docs.append(get_top_docs(claim))\n",
    "    \n",
    "print(claim_top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fae004",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(claim_top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e5dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1]['claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_top_sents(doc_id, claim):\n",
    "    doc_json = db.get_doc_json(doc_id)\n",
    "    page = WikiPage(doc_json['title'], doc_json)\n",
    "    sents = extract_sents(doc_json)\n",
    "    sent_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english')\n",
    "    sent_wm = sent_vectorizer.fit_transform(sents)\n",
    "    claim_tfidf = sent_vectorizer.transform([claim])\n",
    "    cosine_similarities = cosine_similarity(claim_tfidf, sent_wm).flatten()\n",
    "    top_sents_indices = cosine_similarities.argsort()[:-6:-1]\n",
    "    print(top_sents_indices)\n",
    "    return [sent for i, sent in enumerate(sents) if i in top_sents_indices]\n",
    "    \n",
    "claim = train_data[0]['claim']\n",
    "doc_id = 'Tammy Garcia'\n",
    "print(get_top_sents(doc_id, claim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a52d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75bbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_json = db.get_doc_json(doc_id)\n",
    "pp.pprint(doc_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf3059",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
