{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d755f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import jsonlines\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "sys.path.insert(0, \"e:\\\\Documents\\\\NLP\\\\FEVER2021_SharedTask\\\\FEVEROUS\\\\src\")\n",
    "\n",
    "DIR_PATH = \"e:\\\\Documents\\\\NLP\\\\FEVER2021_SharedTask\\\\\"\n",
    "TRAIN_DATA_PATH = os.path.join(DIR_PATH, 'data\\\\train.jsonl')\n",
    "\n",
    "from database.feverous_db import FeverousDB\n",
    "from utils.wiki_page import WikiPage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cced2",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249fbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FeverousDB(\"C:/Databases/feverous_wikiv1.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_entities(sent):\n",
    "    regex = r'\\[\\[([^\\|]+)\\|([^\\]]+)\\]\\]'\n",
    "    return re.sub(regex, '\\\\2', sent)\n",
    "  \n",
    "def remove_punctuation(sent):\n",
    "    if sent[-1] == '.':\n",
    "        return sent[:-1]\n",
    "    else:\n",
    "        return sent\n",
    "\n",
    "def extract_sents(doc_json):\n",
    "    page = WikiPage(doc_json['title'], doc_json)\n",
    "    sents = [replace_entities(sent.content) for sent in page.get_sentences()]\n",
    "    sents = [sent.lower() for sent in sents]\n",
    "    # sents = [remove_punctuation(sent) for sent in sents]\n",
    "    return sents\n",
    "\n",
    "def create_sample_docs(ids):\n",
    "    json_docs = db.get_doc_jsons(ids)\n",
    "    curr_sample_docs = dict()\n",
    "    for doc in json_docs:\n",
    "        sents = extract_sents(doc)\n",
    "        doc_text = ' '.join(sents)\n",
    "        curr_sample_docs[doc['title']] = doc_text\n",
    "    return curr_sample_docs\n",
    "\n",
    "def write_corpora_to_file(i, docs):\n",
    "    start_time = time.time()\n",
    "    FILE_PATH = 'data\\\\corpora_{}.json'.format(i)\n",
    "    with open(DIR_PATH + FILE_PATH, 'w') as f:\n",
    "        f.write(json.dumps(docs, indent=2))\n",
    "    print(\"Wrote {} docs to {}: {} seconds\".format(SAMPLE_SIZE, FILE_PATH, time.time()-start_time))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca70b12",
   "metadata": {},
   "source": [
    "### Create sample docs using multiple threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be310ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "\n",
    "NR_OF_THREADS = 1\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    thread_samples = int(SAMPLE_SIZE / NR_OF_THREADS)\n",
    "    start_time = time.time()\n",
    "    futures = []\n",
    "    for i in range(NR_OF_THREADS):\n",
    "        start = thread_samples*i\n",
    "        ids = sample_doc_ids[start:start+thread_samples]\n",
    "        futures.append(executor.submit(create_sample_docs, ids))\n",
    "\n",
    "sample_docs = dict()\n",
    "\n",
    "for f in futures:\n",
    "    sample_docs.update(f.result())\n",
    "\n",
    "print(\"Creating {} sample docs with {} threads: {} seconds\".format(SAMPLE_SIZE, NR_OF_THREADS, time.time()-start_time))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52c17f7",
   "metadata": {},
   "source": [
    "### Create sample docs on a single thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb0905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sample_docs = create_sample_docs(sample_doc_ids)\n",
    "print(\"Creating {} sample docs: {} seconds\".format(SAMPLE_SIZE, time.time()-start_time))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a46c90",
   "metadata": {},
   "source": [
    "### Create corpora docs for full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce811897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "SAMPLE_SIZE = 100000\n",
    "NR_OF_DOCS = len(doc_ids)\n",
    "iterations = int(math.ceil(NR_OF_DOCS/SAMPLE_SIZE))\n",
    "\n",
    "for i in range(iterations):\n",
    "    start = i*SAMPLE_SIZE\n",
    "    end = (i+1)*SAMPLE_SIZE\n",
    "    if end > NR_OF_DOCS:\n",
    "        end = NR_OF_DOCS\n",
    "    print(\"Creating docs for samples {} to {}\".format(start, end))    \n",
    "    start_time = time.time()    \n",
    "    ids = doc_ids[start:end]\n",
    "    docs = create_sample_docs(ids)\n",
    "    print(\"Creating {} docs took: {}\".format(SAMPLE_SIZE, time.time()-start_time))\n",
    "    write_corpora_to_file(i+1, docs)\n",
    "\n",
    "print(\"Finished creating corpora files!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
